---
title: "Image Retrieval"
date: today
date-format: long
author: "Steven  Ndung'u et al."
format:
  html:
    toc: false
    toc-depth: 3
    toc-location: left
    page-layout: full
    theme:
          light: flatly
          dark: darkly
    number-sections: false
    highlighting: true
    smooth-scroll: true
    code-fold: true
    highlighting-style: github
    self-contained: true
execute:
    echo: true
    warning: false
    enable: true

title-block-banner: true

---

```{=html}
<style type="text/css">

h1.title {
  font-size: 20px;
  color: White;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 16px;
  font-family: "Source Sans Pro Semibold", Times, serif;
  color: Red;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 16px;
  font-family: "Source Sans Pro Semibold", Times, serif;
  color: Red;
  text-align: center;
}
</style>
```

------------------------------------------------------------------------
:::{.column-page}

::: {style="text-align:center"}
<h2>Model Evaluation: Densenet161</h2>
:::

</br>

### Introduction

This script is aimed to evaluate a pre-trained image retrieval model based on the DenseNet161 architecture on test dataset.


### Binarization and Mean Average Precision (mAP)

Mean average precision (mAP) is a commonly used evaluation metric in image retrieval tasks. It measures the average precision across all queries in the dataset. Precision is defined as the number of relevant images retrieved divided by the total number of images retrieved.



The formula for MAP is given as:


\begin{equation}
\text{AP} = \frac{1}{\text{GTP} }\sum_{i=1}^{n}\text{Precision}(i)\times\text{Rel}(i),
\end{equation}

\begin{equation}
\text{mAP} = \frac{1}{N_q }\sum_{j=1}^{N_q}AP_j,
\end{equation}


where AP represents the average precision of one query, with $n$ being the total number of reference images, and $\text{GTP}$ the  total number of ground truth positives, $\text{Precision}(i)$ is the precision of the top $i$ ranked reference images and $\text{Rel}(i)$ is an indicator variable that is 1 if the $i$th image is relevant and 0 otherwise. Finally, the mAP is computed as the average of all AP values obtained for all $N_q$ query images.

::: {.callout-tip}

- In image retrieval, a query is typically an image, and the task is to retrieve a set of relevant images from a large dataset. The mAP metric is used to evaluate the performance of the retrieval system by comparing the retrieved images to a set of ground-truth relevant images for each query.

- mAP takes into account both the relevance and the ranking of the retrieved images. A high mAP score indicates that the retrieval system is able to retrieve a high proportion of relevant images, and that these images are ranked highly in the retrieved set.
:::



```{python}

#Model Evaluation: Densenet161
#source:https://www.alanshawn.com/tech/2022/05/16/matplotlib-latex-guide.html

#These two scripts can be obtained from my github account: https://github.com/stevenndungu/deep_hashing
from python_modules import *
from cosfire_workflow_utils import *


#log_file: slurm-28498889.out
#complete history and code: https://www.comet.com/stevenndungu/image-retrieval-deep-hashing-v1-0-densenet161/3688bbfa63434b6aaef8702d33a07f41


class CustomHead(nn.Module):
   def __init__(self, in_features, out_features):
      super().__init__()
      self.hd = nn.Sequential(AdaptiveConcatPool2d(),
                        Flatten(),
                        nn.BatchNorm1d(in_features),
                        nn.Dropout(p = 0.5),
                        nn.Linear(in_features, 512),
                        nn.ReLU(inplace = True),
                        nn.BatchNorm1d(512),
                        nn.Dropout(p = 0.2),
                        nn.Linear(512, out_features),
                        nn.Sigmoid()
                        )
      
   def forward(self, x):
      return self.hd(x)


loss_function = losses.TripletMarginLoss(margin=0.2,
                                          distance = CosineSimilarity(),
                                          reducer = ThresholdReducer(high=0.3), 
                                          embedding_regularizer = LpRegularizer())


#Data loading
path = Path('../data-balanced')
bs = 32

dls = DataBlock(blocks = (ImageBlock,CategoryBlock),
                  get_items = get_image_files,
                  splitter = GrandparentSplitter(),
                    item_tfms=Resize(224),
                  get_y = parent_label)
#path_img.resolve()
dls = dls.dataloaders(path)


densenet161_model = vision_learner(dls, models.densenet161,
                                       custom_head = CustomHead(4416,8),
                                       loss_func = loss_function,
                                       metrics = error_rate)

model_name = 'IMR_DNT161_model_data-balanced_Tue_Feb__7_09_10_59_2023'

densenet161_model.load('IMR_DNT161_model_data-balanced_Tue_Feb__7_09_10_59_2023')

##############################################################
##############################################################
dic_labels = {'Bent': 0,
                'Compact': 1, 
                'FRI': 2,
                'FRII': 3 
              }

dic_labels_rev = { 2:'Bent',
                3:'Compact',
                  0:'FRI',
                  1: 'FRII'
              }

result_folder = '../' + model_name


def add_data_paths(train_paths = '../data-balanced/train/*/*', dic_labels = dic_labels):
   df_labels_train_paths = pd.DataFrame()
   df_labels_train_paths['paths'] = glob.glob(train_paths)
   df_labels_train_paths['label'] = df_labels_train_paths['paths'].apply(lambda x: x.split(os.path.sep)[1] )
   df_labels_train_paths['label_code'] = df_labels_train_paths['label'].map(dic_labels)
   df_labels_train_paths = df_labels_train_paths.sort_values('label_code')
   df_labels_train_paths = df_labels_train_paths.reset_index()[['paths', 'label', 'label_code']]
   return df_labels_train_paths

#df_labels_train_paths = add_data_paths(train_paths = 'data-balanced/train/*/*', dic_labels = dic_labels)


#Binarize data

def binarize_data(file_path,dic_labels,thresh = 0.5):
  files = get_image_files(file_path)
  train_dl = densenet161_model.dls.test_dl(files)
  preds,y = densenet161_model.get_preds(dl=train_dl)
  db_binary = np.array([list(np.array(out >= np.percentile(out,thresh))*1) for x, out in enumerate(preds)])
  db_label = np.array([str(files[pth]).split('\\')[2] for pth in range(len(files))])
  db_label = np.array([dic_labels[lbl] for x, lbl in enumerate(db_label)])
  
  return preds, db_binary, db_label


#Save/load the the binaries 


if os.path.exists(result_folder +'/df_testing.csv') and os.path.exists(result_folder +'/df_training.csv') and \
    os.path.exists(result_folder +'/df_valid.csv'):
    
    df_training = pd.read_csv(result_folder +'/df_training.csv')
    df_training['predictions'] = [ast.literal_eval(pred) for pred in df_training.predictions]

    df_valid = pd.read_csv(result_folder +'/df_valid.csv')
    df_valid['predictions'] = [ast.literal_eval(pred) for pred in df_valid.predictions]

    df_testing = pd.read_csv(result_folder +'/df_testing.csv')
    df_testing['predictions'] = [ast.literal_eval(pred) for pred in df_testing.predictions]
    #predicted_test_labels = torch.load(result_folder +'/predicted_test_labels')

else:
    thresh = 60  #just a threshold not optimal
    preds_train, train_binary, train_label = binarize_data(file_path = '../data-balanced/train/',
                                                    dic_labels = dic_labels,
                                                    thresh = thresh
                                                    )

    preds_valid,valid_binary, valid_label = binarize_data(file_path = '../data-balanced/valid/',
                                                dic_labels = dic_labels,
                                                    thresh = thresh
                                                    )
    preds_test,test_binary, test_label = binarize_data(file_path = '../data-balanced/test/',
                                                dic_labels = dic_labels,
                                                    thresh = thresh
                                                    )


    if not os.path.isdir(result_folder):
      os.mkdir(result_folder)
    
    flat_predictions_test = []
    for i in range(len(test_label)):
      flat_predictions_test.append(list(np.array(preds_test)[i]))
    df_testing = pd.DataFrame()
    df_testing['predictions'] = flat_predictions_test
    df_testing['label_code'] = test_label
    df_testing['lable_name'] = df_testing['label_code'].map(dic_labels_rev)
    df_testing.to_csv(result_folder +'/df_testing.csv',index = False)

    flat_predictions_valid = []
    for i in range(len(valid_label)):
      flat_predictions_valid.append(list(np.array(preds_valid)[i]))
    df_valid = pd.DataFrame()
    df_valid['predictions'] = flat_predictions_valid
    df_valid['label_code'] = valid_label
    df_valid['lable_name'] = df_valid['label_code'].map(dic_labels_rev)
    df_valid.to_csv(result_folder +'/df_valid.csv',index = False)

    flat_predictions_train = []
    for i in range(len(train_label)):
      flat_predictions_train.append(list(np.array(preds_train)[i]))
    df_training = pd.DataFrame()
    df_training['predictions'] = flat_predictions_train
    df_training['label_code'] = train_label
    df_training['lable_name'] = df_training['label_code'].map(dic_labels_rev)
    df_training.to_csv(result_folder +'/df_training.csv',index = False)


```

###  Model Validation

::: {.panel-tabset}



#### Model performance

```{python}
#Get optimal threshold from validation data 
thresholds = list(range(0,100,5))#[30, 50, 55, 65, 70, 85, 90]#)
# thresholds = np.linspace(50, 70, 50).tolist()

mAP_results = []
for _,thresh in enumerate(thresholds):

  maP,train_binary, train_label, valid_binary, valid_label = mAP_values(df_training,df_valid,thresh, percentile = True,topk=100)
  mAP_results.append(maP)



data = {'mAP': mAP_results,
        'threshold': thresholds}

df = pd.DataFrame(data)

# Find the index of the maximum mAP value
max_map_index = df['mAP'].idxmax()

# Retrieve the threshold corresponding to the maximum mAP
threshold_max_map = df.loc[max_map_index, 'threshold']

maP_valid,train_binary, train_label, valid_binary, valid_label = mAP_values(df_training,df_valid,thresh = threshold_max_map, percentile = True,topk=100)

maP_test,train_binary, train_label, test_binary, test_label = mAP_values(df_training,df_testing,thresh = threshold_max_map, percentile = True, topk=100)

# Plot the line curve
plt.plot(thresholds, mAP_results,  linestyle='-',color = 'red')
plt.xlabel('Threshold (Percentile)')
plt.ylabel('mAP')
plt.show()

print('The optimal threshold is: ', threshold_max_map)
print('The Best Validation mAP is: ',maP_valid)

print('At the optimal threshold: ', threshold_max_map)
print('The Test  mAP is: ',maP_test)
```

#### Model performance (paper)


```{python}
#What is on the paper
data = [json.loads(line) for line in open("../data_generated_IMR_DNT161_model_data-balanced_Tue_Feb__7_09_10_59_2023.json",'r')]

thresh_value_paper = []
maP_paper = []
train_binary_paper = []
test_binary_paper = []

for i in range(len(data)):
    thresh_value_paper.append(data[i]['thresh_value'])
    maP_paper.append(data[i]['maP'])
    train_binary_paper.append(np.array(data[i]['train_binary']))
    test_binary_paper.append(np.array(data[i]['test_binary']))


plt.plot(thresh_value_paper, maP_paper,  linestyle='-',color = 'red')
plt.xlabel('Threshold (Percentile)')
plt.ylabel('mAP')
plt.show()

# df_values = pd.DataFrame({"thresh_value" : thresh_value,
#                     "maP" : maP,
#                     "train_binary" : train_binary,
#                     "test_binary" : test_binary
#                     })

```

:::

### View the image retrieval - COSFIRE Approach


```{python}


def add_data_paths(train_paths = '../data-balanced/train/*/*', dic_labels = dic_labels):
   df_labels_train_paths = pd.DataFrame()
   df_labels_train_paths['paths'] = glob.glob(train_paths)
   df_labels_train_paths['label'] = df_labels_train_paths['paths'].apply(lambda x: x.split(os.path.sep)[1] )
   df_labels_train_paths['label_code'] = df_labels_train_paths['label'].map(dic_labels)
   df_labels_train_paths = df_labels_train_paths.sort_values('label_code')
   df_labels_train_paths = df_labels_train_paths.reset_index()[['paths', 'label', 'label_code']]
   return df_labels_train_paths

df_labels_train_paths = add_data_paths(train_paths = '../data-balanced/train/*/*', dic_labels = dic_labels)
df_labels_test_paths = add_data_paths(train_paths = '../data-balanced/test/*/*', dic_labels = dic_labels)

def perf_percentages(input_data):
    unique, counts = np.unique(input_data, return_counts=True)
    df = pd.DataFrame()
    df['unique'] = unique
    df['counts'] = counts
    df['Percentage'] = np.round(counts / counts.sum() * 100)
    return df

    
def query_image(test_image_index = 0, 
               test_images_paths = df_labels_test_paths,
               train_images_db_paths = df_labels_train_paths,
               train_images_db = train_binary,
               test_binary = test_binary):

         
    print('Test Image is: ', test_images_paths.label[test_image_index])
    fig = plt.figure(figsize=(3, 3))
    image_test = Image.open(test_images_paths.paths[test_image_index])
    image_test = torch.from_numpy(np.array(image_test))
    plt.imshow(image_test[:, :, 1], cmap='viridis')
    plt.axis('off')
    plt.show()

    test_image = test_binary[test_image_index]  
    #np.count_nonzero(np.array([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
      # 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0])==np.array([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
      # 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]))
    similarity_distance = np.count_nonzero(test_image != train_images_db, axis=1)
    sort_indices = np.argsort(similarity_distance)
    top_indices = sort_indices[:100]
    #print(top_indices)
    paths_to_imgs = [train_images_db_paths.paths[index] for _,index in enumerate(top_indices)]
    df = perf_percentages([train_images_db_paths.label[index] for index in top_indices])
    print(df)
    cols = 7
    rows = 4

    fig = plt.figure(figsize=(2 * cols, 2 * rows))
    for col in range(cols):
        for i, img_path in enumerate(paths_to_imgs[:cols*rows]):
            ax = fig.add_subplot(rows, cols, i + 1)
            ax.grid(visible=False)
            ax.axis("off")
            image = Image.open(img_path)
            image = torch.from_numpy(np.array(image))
            ax.imshow(image[:, :, 1], cmap='viridis')
            ax.set_title(img_path.split(os.path.sep)[1])

    plt.show()
  
```


::: {.panel-tabset}

#### FRI 


</br>

```{python}
#query_image(test_image_index = random.randint(0, 100))
```

#### FRII


</br>

```{python}

#query_image(test_image_index = random.randint(101, 202))
```

#### Bent

</br>

```{python}

#query_image(test_image_index = random.randint(205, 300))
```
#### Compact



</br>

```{python}

#query_image(test_image_index = random.randint(310, 400))
```


:::
:::
